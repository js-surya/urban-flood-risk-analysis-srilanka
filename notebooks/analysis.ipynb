{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Urban Flood Vulnerability Assessment - Sri Lanka\n",
    "\n",
    "**Assignment 2 - Scientific Programming for Geospatial Sciences**\n",
    "\n",
    "This notebook demonstrates the complete flood vulnerability analysis workflow:\n",
    "1. Data Loading\n",
    "2. NumPy Array Operations (raster processing)\n",
    "3. PyTorch Tensor Operations (with performance comparison)\n",
    "4. Vector Processing (GeoPandas/Shapely)\n",
    "5. Xarray Data Cubes\n",
    "6. Raster-Vector Integration\n",
    "7. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# our modules\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src import data_loading, raster_analysis, tensor_operations, vector_analysis, integration, visualization\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "\n",
    "Load the required datasets for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data paths\n",
    "DATA_DIR = Path('../data')\n",
    "\n",
    "# check if data exists\n",
    "# NOTE: you need to download the data first, see data/README.md\n",
    "print(\"Data directory contents:\")\n",
    "if DATA_DIR.exists():\n",
    "    for f in DATA_DIR.iterdir():\n",
    "        print(f\"  - {f.name}\")\n",
    "else:\n",
    "    print(\"  Data directory not found. Please create it and add data files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: load rainfall data (CHIRPS)\n",
    "# uncomment when you have the data\n",
    "\n",
    "# rainfall = data_loading.load_chirps_data(\n",
    "#     DATA_DIR / 'chirps_2020_srilanka.nc',\n",
    "#     time_slice=('2020-01-01', '2020-12-31')\n",
    "# )\n",
    "# print(f\"Rainfall data shape: {rainfall.shape}\")\n",
    "# print(f\"Rainfall dimensions: {rainfall.dims}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. NumPy Array Operations\n",
    "\n",
    "Demonstrate array-based raster processing operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sample rainfall data for demonstration\n",
    "# in practice this would come from CHIRPS\n",
    "np.random.seed(42)\n",
    "sample_rainfall = np.random.exponential(scale=30, size=(365, 100, 100))\n",
    "print(f\"Sample rainfall shape: {sample_rainfall.shape}\")\n",
    "print(f\"Max value: {sample_rainfall.max():.2f} mm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# operation 1: create extreme rainfall mask\n",
    "extreme_mask = raster_analysis.create_extreme_rainfall_mask(sample_rainfall, threshold=100)\n",
    "print(f\"Extreme rainfall events (>100mm): {extreme_mask.sum()} occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# operation 2: count extreme events per pixel\n",
    "extreme_counts = raster_analysis.count_extreme_events(sample_rainfall, threshold=100)\n",
    "print(f\"Max extreme events at a location: {extreme_counts.max()}\")\n",
    "\n",
    "# visualize\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(extreme_counts, cmap='Reds')\n",
    "plt.colorbar(label='Number of extreme events')\n",
    "plt.title('Extreme Rainfall Events per Location')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# operation 3: calculate 95th percentile rainfall\n",
    "p95_rainfall = raster_analysis.calculate_percentile_rainfall(sample_rainfall, percentile=95)\n",
    "print(f\"95th percentile range: {p95_rainfall.min():.2f} - {p95_rainfall.max():.2f} mm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# operation 4: normalize for vulnerability calculation\n",
    "rainfall_norm = raster_analysis.normalize_array(p95_rainfall, method='minmax')\n",
    "print(f\"Normalized range: {rainfall_norm.min():.2f} - {rainfall_norm.max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PyTorch Tensor Operations\n",
    "\n",
    "Demonstrate GPU-aware tensor operations and performance comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check GPU availability\n",
    "tensor_operations.print_gpu_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to tensor\n",
    "rainfall_tensor = tensor_operations.numpy_to_tensor(p95_rainfall, device='auto')\n",
    "print(f\"Tensor device: {rainfall_tensor.device}\")\n",
    "print(f\"Tensor shape: {rainfall_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply gaussian convolution to find storm centers\n",
    "smoothed_tensor = tensor_operations.apply_gaussian_convolution(\n",
    "    rainfall_tensor, kernel_size=5, sigma=1.5\n",
    ")\n",
    "\n",
    "# convert back for visualization\n",
    "smoothed = tensor_operations.tensor_to_numpy(smoothed_tensor)\n",
    "\n",
    "# compare original vs smoothed\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axes[0].imshow(p95_rainfall, cmap='Blues')\n",
    "axes[0].set_title('Original Rainfall')\n",
    "axes[1].imshow(smoothed, cmap='Blues')\n",
    "axes[1].set_title('Smoothed (PyTorch Convolution)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERFORMANCE COMPARISON: NumPy vs PyTorch\n",
    "# this is required by the assignment\n",
    "\n",
    "print(\"Running performance comparison (this may take a moment)...\")\n",
    "perf_results = tensor_operations.compare_numpy_vs_torch(\n",
    "    p95_rainfall, \n",
    "    kernel_size=5, \n",
    "    sigma=1.5, \n",
    "    num_iterations=10\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PERFORMANCE COMPARISON RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Array size: {p95_rainfall.shape}\")\n",
    "print(f\"Kernel size: 5x5 Gaussian\")\n",
    "print(f\"\")\n",
    "print(f\"NumPy (scipy.ndimage): {perf_results['numpy_time']*1000:.2f} ms ± {perf_results['numpy_std']*1000:.2f} ms\")\n",
    "print(f\"PyTorch ({perf_results['device']}):    {perf_results['torch_time']*1000:.2f} ms ± {perf_results['torch_std']*1000:.2f} ms\")\n",
    "print(f\"\")\n",
    "print(f\"Speedup: {perf_results['speedup']:.2f}x\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Vector Processing (GeoPandas/Shapely)\n",
    "\n",
    "Demonstrate at least 3 required geospatial operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sample data for demonstration\n",
    "# in practice this would come from Google Buildings and OSM\n",
    "\n",
    "from shapely.geometry import box, Point, LineString\n",
    "\n",
    "# sample admin boundaries (3 districts)\n",
    "admin_boundaries = gpd.GeoDataFrame({\n",
    "    'district_id': ['D001', 'D002', 'D003'],\n",
    "    'district_name': ['Colombo', 'Gampaha', 'Kalutara'],\n",
    "    'geometry': [\n",
    "        box(79.8, 6.8, 80.0, 7.0),\n",
    "        box(79.9, 7.0, 80.1, 7.2),\n",
    "        box(79.7, 6.5, 79.9, 6.8)\n",
    "    ]\n",
    "}, crs='EPSG:4326')\n",
    "\n",
    "# sample buildings (100 random points converted to polygons)\n",
    "np.random.seed(42)\n",
    "building_points = [\n",
    "    Point(np.random.uniform(79.7, 80.1), np.random.uniform(6.5, 7.2))\n",
    "    for _ in range(100)\n",
    "]\n",
    "buildings = gpd.GeoDataFrame({\n",
    "    'building_id': [f'B{i:03d}' for i in range(100)],\n",
    "    'geometry': [p.buffer(0.002) for p in building_points]  # make small polygons\n",
    "}, crs='EPSG:4326')\n",
    "\n",
    "# sample roads\n",
    "roads = gpd.GeoDataFrame({\n",
    "    'highway': ['primary', 'secondary', 'primary'],\n",
    "    'geometry': [\n",
    "        LineString([(79.7, 6.8), (80.1, 6.8)]),\n",
    "        LineString([(79.9, 6.5), (79.9, 7.2)]),\n",
    "        LineString([(79.8, 7.0), (80.0, 7.0)])\n",
    "    ]\n",
    "}, crs='EPSG:4326')\n",
    "\n",
    "print(f\"Admin boundaries: {len(admin_boundaries)} districts\")\n",
    "print(f\"Buildings: {len(buildings)} footprints\")\n",
    "print(f\"Roads: {len(roads)} segments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPERATION 1: Spatial Join - assign district to each building\n",
    "buildings_joined = vector_analysis.spatial_join_buildings_to_admin(\n",
    "    buildings, admin_boundaries, admin_id_col='district_id'\n",
    ")\n",
    "print(\"Operation 1: Spatial Join\")\n",
    "print(buildings_joined[['building_id', 'district_id']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPERATION 2: Buffer Analysis - create road buffers\n",
    "road_buffers = vector_analysis.create_road_buffers(\n",
    "    roads, buffer_distance=0.01, road_types=['primary']\n",
    ")\n",
    "print(\"\\nOperation 2: Buffer Analysis\")\n",
    "print(f\"Created {len(road_buffers)} road buffers\")\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "admin_boundaries.plot(ax=ax, alpha=0.3, edgecolor='black')\n",
    "road_buffers.plot(ax=ax, alpha=0.5, color='yellow')\n",
    "buildings.plot(ax=ax, color='red', markersize=5)\n",
    "ax.set_title('Road Buffers and Buildings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPERATION 3: Density Calculation\n",
    "admin_with_density = vector_analysis.calculate_building_density(\n",
    "    buildings, admin_boundaries, admin_id_col='district_id'\n",
    ")\n",
    "print(\"\\nOperation 3: Building Density Calculation\")\n",
    "print(admin_with_density[['district_name', 'building_count', 'building_density']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Xarray Data Cubes\n",
    "\n",
    "Demonstrate multi-dimensional data handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sample xarray dataset (simulating CHIRPS)\n",
    "times = pd.date_range('2020-01-01', periods=365, freq='D')\n",
    "lats = np.linspace(6.5, 7.2, 50)\n",
    "lons = np.linspace(79.7, 80.1, 50)\n",
    "\n",
    "# create rainfall data cube\n",
    "np.random.seed(42)\n",
    "rainfall_data = np.random.exponential(scale=20, size=(365, 50, 50))\n",
    "\n",
    "rainfall_cube = xr.DataArray(\n",
    "    data=rainfall_data,\n",
    "    dims=['time', 'latitude', 'longitude'],\n",
    "    coords={\n",
    "        'time': times,\n",
    "        'latitude': lats,\n",
    "        'longitude': lons\n",
    "    },\n",
    "    name='precipitation'\n",
    ")\n",
    "\n",
    "print(\"Data Cube Information:\")\n",
    "print(rainfall_cube)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporal operations\n",
    "\n",
    "# annual maximum\n",
    "annual_max = rainfall_cube.groupby('time.year').max(dim='time')\n",
    "print(f\"Annual maximum shape: {annual_max.shape}\")\n",
    "\n",
    "# monthly mean\n",
    "monthly_mean = rainfall_cube.groupby('time.month').mean(dim='time')\n",
    "print(f\"Monthly mean shape: {monthly_mean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporal slicing - extract monsoon season\n",
    "monsoon = rainfall_cube.sel(time=slice('2020-05-01', '2020-09-30'))\n",
    "print(f\"Monsoon period: {monsoon.time.min().values} to {monsoon.time.max().values}\")\n",
    "print(f\"Monsoon days: {len(monsoon.time)}\")\n",
    "print(f\"Max monsoon rainfall: {monsoon.max().values:.2f} mm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial aggregation\n",
    "total_rainfall = rainfall_cube.sum(dim='time')\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "total_rainfall.plot(cmap='Blues')\n",
    "plt.title('Total Annual Rainfall (mm)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Raster-Vector Integration\n",
    "\n",
    "The core requirement: bidirectional raster-vector operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for demonstration, save sample raster to file\n",
    "import rasterio\n",
    "from rasterio.transform import from_bounds\n",
    "\n",
    "# save annual max as geotiff\n",
    "output_dir = Path('../outputs')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "bounds = (79.7, 6.5, 80.1, 7.2)\n",
    "transform = from_bounds(*bounds, 50, 50)\n",
    "\n",
    "rainfall_sample = p95_rainfall\n",
    "raster_path = output_dir / 'sample_rainfall.tif'\n",
    "\n",
    "with rasterio.open(\n",
    "    raster_path, 'w',\n",
    "    driver='GTiff',\n",
    "    height=100, width=100,\n",
    "    count=1, dtype='float32',\n",
    "    crs='EPSG:4326',\n",
    "    transform=from_bounds(*bounds, 100, 100)\n",
    ") as dst:\n",
    "    dst.write(rainfall_sample.astype('float32'), 1)\n",
    "\n",
    "print(f\"Saved sample raster to: {raster_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RASTER -> VECTOR: Zonal Statistics\n",
    "admin_with_rainfall = integration.extract_zonal_statistics(\n",
    "    admin_boundaries,\n",
    "    raster_path,\n",
    "    stats=['mean', 'max', 'min'],\n",
    "    prefix='rainfall_'\n",
    ")\n",
    "\n",
    "print(\"Zonal Statistics (Raster -> Vector):\")\n",
    "print(admin_with_rainfall[['district_name', 'rainfall_mean', 'rainfall_max']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VECTOR -> RASTER: Rasterize building density\n",
    "# first calculate building density\n",
    "admin_with_density = vector_analysis.calculate_building_density(\n",
    "    buildings, admin_boundaries, admin_id_col='district_id'\n",
    ")\n",
    "\n",
    "# rasterize\n",
    "density_raster = integration.rasterize_vector(\n",
    "    admin_with_density,\n",
    "    value_column='building_density',\n",
    "    resolution=(-0.01, 0.01)\n",
    ")\n",
    "\n",
    "print(\"Rasterized Building Density (Vector -> Raster):\")\n",
    "print(f\"Shape: {density_raster.shape}\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "density_raster.plot(cmap='Reds')\n",
    "plt.title('Building Density Raster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization\n",
    "\n",
    "Create maps and charts for the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add vulnerability score for visualization\n",
    "# (in practice this would come from the full integration pipeline)\n",
    "admin_with_density['vulnerability_score'] = np.random.uniform(0.3, 0.9, len(admin_with_density))\n",
    "admin_with_density['id'] = admin_with_density['district_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create interactive vulnerability map\n",
    "vuln_map = visualization.create_vulnerability_map(\n",
    "    admin_with_density,\n",
    "    value_column='vulnerability_score',\n",
    "    title='Flood Vulnerability Score'\n",
    ")\n",
    "\n",
    "# display in notebook\n",
    "vuln_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save map to HTML\n",
    "vuln_map.save(output_dir / 'vulnerability_map.html')\n",
    "print(f\"Map saved to: {output_dir / 'vulnerability_map.html'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create ranking chart\n",
    "ranking_chart = visualization.create_vulnerability_ranking_chart(\n",
    "    admin_with_density,\n",
    "    name_column='district_name',\n",
    "    value_column='vulnerability_score',\n",
    "    top_n=10,\n",
    "    title='Top Vulnerable Districts'\n",
    ")\n",
    "\n",
    "ranking_chart.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create static map for report\n",
    "fig = visualization.create_static_map(\n",
    "    admin_with_density,\n",
    "    value_column='vulnerability_score',\n",
    "    title='Flood Vulnerability Assessment - Sri Lanka'\n",
    ")\n",
    "\n",
    "# save for report\n",
    "fig.savefig(output_dir / 'vulnerability_map.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"Figure saved to: {output_dir / 'vulnerability_map.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated all required technical components:\n",
    "\n",
    "| Component | Implementation |\n",
    "|-----------|----------------|\n",
    "| NumPy Arrays | Masking, normalization, percentile calculation |\n",
    "| PyTorch Tensors | Gaussian convolution with GPU awareness, performance comparison |\n",
    "| Vector Processing | Spatial join, buffer analysis, density calculation (3+ operations) |\n",
    "| Xarray Data Cubes | Temporal slicing, aggregation, groupby operations |\n",
    "| Raster-Vector Integration | Zonal statistics (R→V), rasterization (V→R) |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
