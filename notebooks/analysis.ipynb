{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Urban Flood Vulnerability Assessment - Colombo District, Sri Lanka\n",
    "\n",
    "**Assignment 2 - Scientific Programming for Geospatial Sciences**\n",
    "\n",
    "**Authors:** Surya Jamuna Rani Subramaniyan (S3664414) & Sachin Ravi (S3563545)\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "0. **Setup & Data Download** - Automated data acquisition\n",
    "1. **Data Loading** - Load real datasets (DEM, CHIRPS, Buildings)\n",
    "2. **NumPy Array Operations** - Raster processing\n",
    "3. **PyTorch Tensor Operations** - GPU-aware processing with performance comparison\n",
    "4. **Vector Processing** - GeoPandas/Shapely operations (3+)\n",
    "5. **Xarray Data Cubes** - Multi-temporal analysis\n",
    "6. **Raster-Vector Integration** - Bidirectional operations\n",
    "7. **Visualization** - Maps and dashboard\n",
    "\n",
    "---\n",
    "\n",
    "**Study Area:** Colombo District, Sri Lanka  \n",
    "**Bounding Box:** 79.82E - 80.22E, 6.75N - 7.05N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup & Data Download\n",
    "\n",
    "Run this section once to download all required datasets automatically.  \n",
    "**No API keys required!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import json\n",
    "import gzip\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Our modules\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src import data_loading, raster_analysis, tensor_operations, vector_analysis, integration, visualization\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories ready\n"
     ]
    }
   ],
   "source": [
    "# Study area configuration\n",
    "COLOMBO_BBOX = {\n",
    "    'west': 79.82,\n",
    "    'east': 80.22,\n",
    "    'south': 6.75,\n",
    "    'north': 7.05\n",
    "}\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path('../data')\n",
    "RAW_DIR = DATA_DIR / 'raw'\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "OUTPUT_DIR = Path('../outputs')\n",
    "\n",
    "# Create directories\n",
    "for d in [RAW_DIR / 'chirps', RAW_DIR / 'dem', RAW_DIR / 'admin', \n",
    "          RAW_DIR / 'buildings', PROCESSED_DIR, OUTPUT_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Directories ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for downloads\n",
    "def download_file(url, output_path, timeout=120):\n",
    "    \"\"\"Download a file with progress indication.\"\"\"\n",
    "    if output_path.exists():\n",
    "        print(f\"  Already exists: {output_path.name}\")\n",
    "        return True\n",
    "    \n",
    "    try:\n",
    "        print(f\"  Downloading: {output_path.name}...\")\n",
    "        response = requests.get(url, stream=True, timeout=timeout)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        downloaded = 0\n",
    "        \n",
    "        with open(output_path, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "                downloaded += len(chunk)\n",
    "                if total_size > 0:\n",
    "                    pct = (downloaded / total_size) * 100\n",
    "                    print(f\"\\r  Downloading: {output_path.name}... {pct:.1f}%\", end='', flush=True)\n",
    "        \n",
    "        print(f\"\\n  Saved: {output_path.name}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Download SRTM DEM from AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading SRTM DEM from AWS...\n",
      "  Already exists: N06E079.hgt.gz\n",
      "  Already exists: N06E080.hgt.gz\n",
      "  Already exists: N07E079.hgt.gz\n",
      "  Already exists: N07E080.hgt.gz\n",
      "\n",
      "Downloaded 4 DEM tiles\n"
     ]
    }
   ],
   "source": [
    "# Download SRTM tiles from AWS Open Data\n",
    "print(\"Downloading SRTM DEM from AWS...\")\n",
    "\n",
    "# Get required tiles for Colombo\n",
    "srtm_tiles = ['N06E079', 'N06E080', 'N07E079', 'N07E080']\n",
    "base_url = \"https://elevation-tiles-prod.s3.amazonaws.com/skadi\"\n",
    "\n",
    "dem_files = []\n",
    "for tile in srtm_tiles:\n",
    "    lat_dir = tile[:3]\n",
    "    filename = f\"{tile}.hgt.gz\"\n",
    "    url = f\"{base_url}/{lat_dir}/{filename}\"\n",
    "    output_path = RAW_DIR / 'dem' / filename\n",
    "    \n",
    "    if download_file(url, output_path, timeout=120):\n",
    "        dem_files.append(output_path)\n",
    "\n",
    "print(f\"\\nDownloaded {len(dem_files)} DEM tiles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Download CHIRPS 2025 Monthly Rainfall Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading CHIRPS 2025 monthly rainfall data...\n",
      "  Already exists: chirps-v2.0.2025.monthly.nc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download CHIRPS 2025 monthly rainfall data (~40 MB)\n",
    "print(\"Downloading CHIRPS 2025 monthly rainfall data...\")\n",
    "\n",
    "chirps_url = \"https://data.chc.ucsb.edu/products/CHIRPS-2.0/global_monthly/netcdf/byYear/chirps-v2.0.2025.monthly.nc\"\n",
    "chirps_path = RAW_DIR / 'chirps' / 'chirps-v2.0.2025.monthly.nc'\n",
    "\n",
    "download_file(chirps_url, chirps_path, timeout=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Download Admin Boundaries from OSM",
    "\n\n# Load into memory for masking\n",
    "if admin_path.exists():\n",
    "    try:\n",
    "        district_mask = gpd.read_file(admin_path)\n",
    "        # Dissolve to ensure single polygon\n",
    "        district_mask = district_mask.dissolve()\n",
    "        print(\"District mask loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load district mask: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading admin boundaries from OpenStreetMap...\n",
      "  Already exists: colombo_boundary.json\n"
     ]
    }
   ],
   "source": [
    "# Download Colombo District boundary using Overpass API\n",
    "print(\"Downloading admin boundaries from OpenStreetMap...\")\n",
    "\n",
    "admin_path = RAW_DIR / 'admin' / 'colombo_boundary.json'\n",
    "\n",
    "if not admin_path.exists():\n",
    "    overpass_url = \"https://overpass-api.de/api/interpreter\"\n",
    "    \n",
    "    # Query for Colombo District and its subdivisions\n",
    "    query = f\"\"\"\n",
    "    [out:json][timeout:120];\n",
    "    (\n",
    "      relation[\"name\"~\"Colombo\"][\"admin_level\"~\"5|6|7\"]\n",
    "        ({COLOMBO_BBOX['south']},{COLOMBO_BBOX['west']},{COLOMBO_BBOX['north']},{COLOMBO_BBOX['east']});\n",
    "    );\n",
    "    out geom;\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(overpass_url, data={'data': query}, timeout=180)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        with open(admin_path, 'w') as f:\n",
    "            json.dump(data, f)\n",
    "        \n",
    "        print(f\"  Saved: {admin_path.name}\")\n",
    "        print(f\"  Found {len(data.get('elements', []))} boundary elements\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "else:\n",
    "    print(f\"  Already exists: {admin_path.name}\")",
    "\n\n# Load into memory for masking\n",
    "if admin_path.exists():\n",
    "    try:\n",
    "        district_mask = gpd.read_file(admin_path)\n",
    "        # Dissolve to ensure single polygon\n",
    "        district_mask = district_mask.dissolve()\n",
    "        print(\"District mask loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load district mask: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Download OSM buildings (this may take 5-10 minutes for urban areas)\n",
    "print(\"Downloading building footprints from OpenStreetMap...\")\n",
    "\n",
    "buildings_path = RAW_DIR / 'buildings' / 'osm_buildings.json'\n",
    "\n",
    "if not buildings_path.exists():\n",
    "    overpass_url = \"https://overpass-api.de/api/interpreter\"\n",
    "    \n",
    "    # Query for buildings in Colombo BBox\n",
    "    buildings_query = f\"\"\"\n",
    "    [out:json][timeout:300];\n",
    "    (\n",
    "      way[\"building\"]({COLOMBO_BBOX['south']},{COLOMBO_BBOX['west']},{COLOMBO_BBOX['north']},{COLOMBO_BBOX['east']});\n",
    "    );\n",
    "    out geom;\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(\"  Querying Overpass API...\")\n",
    "        response = requests.post(overpass_url, data={'data': buildings_query}, timeout=600)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        with open(buildings_path, 'w') as f:\n",
    "            json.dump(data, f)\n",
    "        \n",
    "        print(f\"  Saved: {buildings_path.name}\")\n",
    "        print(f\"  Found {len(data.get('elements', []))} building elements\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "else:\n",
    "    print(f\"  Already exists: {buildings_path.name}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download OSM buildings (this may take 5-10 minutes for urban areas)\n",
    "print(\"Downloading building footprints from OpenStreetMap...\")\n",
    "\n",
    "buildings_path = RAW_DIR / 'buildings' / 'osm_buildings.json'\n",
    "\n",
    "if not buildings_path.exists():\n",
    "    overpass_url = \"https://overpass-api.de/api/interpreter\"\n",
    "    \n",
    "    # Query for buildings in Colombo BBox\n",
    "    buildings_query = f\"\"\"\n",
    "    [out:json][timeout:300];\n",
    "    (\n",
    "      way[\"building\"]({COLOMBO_BBOX['south']},{COLOMBO_BBOX['west']},{COLOMBO_BBOX['north']},{COLOMBO_BBOX['east']});\n",
    "    );\n",
    "    out geom;\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(\"  Querying Overpass API...\")\n",
    "        response = requests.post(overpass_url, data={'data': buildings_query}, timeout=600)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        with open(buildings_path, 'w') as f:\n",
    "            json.dump(data, f)\n",
    "        \n",
    "        print(f\"  Saved: {buildings_path.name}\")\n",
    "        print(f\"  Found {len(data.get('elements', []))} building elements\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "else:\n",
    "    print(f\"  Already exists: {buildings_path.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.5 Preview Buildings on Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading OSM Buildings...\n"
     ]
    },
    {
     "ename": "DriverError",
     "evalue": "Failed to open dataset (flags=68): ../data/raw/buildings/osm_buildings.json",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCPLE_OpenFailedError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mfiona/ogrext.pyx:130\u001b[39m, in \u001b[36mfiona.ogrext.gdal_open_vector\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mfiona/ogrext.pyx:134\u001b[39m, in \u001b[36mfiona.ogrext.gdal_open_vector\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mfiona/_err.pyx:375\u001b[39m, in \u001b[36mfiona._err.StackChecker.exc_wrap_pointer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mCPLE_OpenFailedError\u001b[39m: `../data/raw/buildings/osm_buildings.json' not recognized as being in a supported file format.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mDriverError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m osm_buildings_file = RAW_DIR / \u001b[33m'\u001b[39m\u001b[33mbuildings\u001b[39m\u001b[33m'\u001b[39m / \u001b[33m'\u001b[39m\u001b[33mosm_buildings.json\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m osm_buildings_file.exists():\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     buildings = \u001b[43mdata_loading\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_osm_buildings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mosm_buildings_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# Clip to District (Masking requirement)\u001b[39;00m\n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# Check if district_mask is available (defined in earlier cell)\u001b[39;00m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mdistrict_mask\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ITC/Sci Prog/asignment2/src/data_loading.py:226\u001b[39m, in \u001b[36mload_osm_buildings\u001b[39m\u001b[34m(filepath, bbox)\u001b[39m\n\u001b[32m    224\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m gpd.read_file(filepath, bbox=bbox)\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m226\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/anaconda3/envs/geoai/lib/python3.11/site-packages/geopandas/io/file.py:289\u001b[39m, in \u001b[36m_read_file\u001b[39m\u001b[34m(filename, bbox, mask, rows, engine, **kwargs)\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    287\u001b[39m         path_or_bytes = filename\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read_file_fiona\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    294\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33munknown engine \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mengine\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/anaconda3/envs/geoai/lib/python3.11/site-packages/geopandas/io/file.py:315\u001b[39m, in \u001b[36m_read_file_fiona\u001b[39m\u001b[34m(path_or_bytes, from_bytes, bbox, mask, rows, where, **kwargs)\u001b[39m\n\u001b[32m    312\u001b[39m     reader = fiona.open\n\u001b[32m    314\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m fiona_env():\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mreader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m features:\n\u001b[32m    316\u001b[39m         crs = features.crs_wkt\n\u001b[32m    317\u001b[39m         \u001b[38;5;66;03m# attempt to get EPSG code\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/anaconda3/envs/geoai/lib/python3.11/site-packages/fiona/env.py:457\u001b[39m, in \u001b[36mensure_env_with_credentials.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwds)\u001b[39m\n\u001b[32m    454\u001b[39m     session = DummySession()\n\u001b[32m    456\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m env_ctor(session=session):\n\u001b[32m--> \u001b[39m\u001b[32m457\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/anaconda3/envs/geoai/lib/python3.11/site-packages/fiona/__init__.py:342\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(fp, mode, driver, schema, crs, encoding, layer, vfs, enabled_drivers, crs_wkt, ignore_fields, ignore_geometry, include_fields, wkt_version, allow_unsupported_drivers, opener, **kwargs)\u001b[39m\n\u001b[32m    339\u001b[39m         path = _parse_path(fp)\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33ma\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m     colxn = \u001b[43mCollection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_fields\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_fields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m        \u001b[49m\u001b[43minclude_fields\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_fields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_geometry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_geometry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwkt_version\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwkt_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m        \u001b[49m\u001b[43menabled_drivers\u001b[49m\u001b[43m=\u001b[49m\u001b[43menabled_drivers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_unsupported_drivers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_unsupported_drivers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    357\u001b[39m     colxn = Collection(\n\u001b[32m    358\u001b[39m         path,\n\u001b[32m    359\u001b[39m         mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    372\u001b[39m         **kwargs\n\u001b[32m    373\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/anaconda3/envs/geoai/lib/python3.11/site-packages/fiona/collection.py:226\u001b[39m, in \u001b[36mCollection.__init__\u001b[39m\u001b[34m(self, path, mode, driver, schema, crs, encoding, layer, vsi, archive, enabled_drivers, crs_wkt, ignore_fields, ignore_geometry, include_fields, wkt_version, allow_unsupported_drivers, **kwargs)\u001b[39m\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode == \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    225\u001b[39m     \u001b[38;5;28mself\u001b[39m.session = Session()\n\u001b[32m--> \u001b[39m\u001b[32m226\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33ma\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    228\u001b[39m     \u001b[38;5;28mself\u001b[39m.session = WritingSession()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mfiona/ogrext.pyx:876\u001b[39m, in \u001b[36mfiona.ogrext.Session.start\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mfiona/ogrext.pyx:136\u001b[39m, in \u001b[36mfiona.ogrext.gdal_open_vector\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mDriverError\u001b[39m: Failed to open dataset (flags=68): ../data/raw/buildings/osm_buildings.json"
     ]
    }
   ],
   "source": [
    "# Load and Clip OSM Buildings\n",
    "print(\"Loading OSM Buildings...\")\n",
    "osm_buildings_file = RAW_DIR / 'buildings' / 'osm_buildings.json'\n",
    "\n",
    "if osm_buildings_file.exists():\n",
    "    buildings = data_loading.load_osm_buildings(osm_buildings_file)\n",
    "    \n",
    "    # Clip to District (Masking requirement)\n",
    "    # Check if district_mask is available (defined in earlier cell)\n",
    "    if 'district_mask' in locals():\n",
    "        try:\n",
    "            print(\"Clipping buildings to district boundary...\")\n",
    "            buildings = vector_analysis.clip_vectors_to_boundary(buildings, district_mask)\n",
    "        except Exception as e:\n",
    "            print(f\"Clipping failed: {e}\")\n",
    "    \n",
    "    print(f\"Loaded {len(buildings)} buildings\")\n",
    "else:\n",
    "    print(\"OSM Buildings file not found. Please run Section 0.4.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.5 Data Status Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what data we have\n",
    "print(\"DATA STATUS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# DEM\n",
    "dem_files_found = list((RAW_DIR / 'dem').glob('*.hgt.gz'))\n",
    "print(f\"DEM tiles:      {len(dem_files_found)} files\" if dem_files_found else \"DEM tiles:      Not found\")\n",
    "\n",
    "# Admin boundaries\n",
    "admin_files = list((RAW_DIR / 'admin').glob('*.json'))\n",
    "print(f\"Admin boundary: Found\" if admin_files else \"Admin boundary: Not found\")\n",
    "\n",
    "# CHIRPS\n",
    "chirps_files = list((RAW_DIR / 'chirps').glob('*.nc'))\n",
    "print(f\"CHIRPS data:    Found ({chirps_files[0].name})\" if chirps_files else \"CHIRPS data:    Not found\")\n",
    "\n",
    "# Buildings\n",
    "building_files = list((RAW_DIR / 'buildings').glob('*.json'))\n",
    "print(f\"Buildings:      Found\" if building_files else \"Buildings:      Not found\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Data Loading\n",
    "\n",
    "Load the real datasets for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure District Mask is Loaded (Resilience Check)\n",
    "# This ensures the mask is available even if Section 0 was skipped\n",
    "if 'district_mask' not in locals():\n",
    "    admin_path = RAW_DIR / 'admin' / 'colombo_boundary.json'\n",
    "    if admin_path.exists():\n",
    "        try:\n",
    "            print(\"Loading district mask from disk...\")\n",
    "            district_mask = gpd.read_file(admin_path).dissolve()\n",
    "            print(\"District mask loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading district mask: {e}\")\n",
    "    else:\n",
    "        print(\"Warning: District boundary file not found. Visualizations may not be masked.\")\n",
    "else:\n",
    "    print(\"District mask is already loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask Elevation with District Boundary\n",
    "elevation = data_loading.load_srtm_tiles(RAW_DIR / 'dem', bbox=tuple(COLOMBO_BBOX.values()))\n",
    "\n",
    "# Apply Mask\n",
    "elevation = raster_analysis.mask_raster_with_vector(elevation, district_mask)\n",
    "\n",
    "print(f\"Elevation data loaded and masked: {elevation.shape}\")\n",
    "elevation.plot(cmap='terrain', figsize=(10, 8), cbar_kwargs={'label': 'Elevation (m)'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CHIRPS 2025 rainfall data and clip to study area\n",
    "print(\"Loading CHIRPS 2025 monthly rainfall data...\")\n",
    "\n",
    "chirps_file = RAW_DIR / 'chirps' / 'chirps-v2.0.2025.monthly.nc'\n",
    "\n",
    "if chirps_file.exists():\n",
    "    # Load full dataset\n",
    "    ds = xr.open_dataset(chirps_file)\n",
    "    \n",
    "    # Get the precipitation variable and clip to BBOX first\n",
    "    # Slice latitude (handle both ascending and descending cases)\n",
    "    rainfall_cube = ds['precip'].sel(\n",
    "        latitude=slice(COLOMBO_BBOX['south'], COLOMBO_BBOX['north']),\n",
    "        longitude=slice(COLOMBO_BBOX['west'], COLOMBO_BBOX['east'])\n",
    "    )\n",
    "    if rainfall_cube.size == 0:\n",
    "       rainfall_cube = ds['precip'].sel(\n",
    "          latitude=slice(COLOMBO_BBOX['north'], COLOMBO_BBOX['south']),\n",
    "          longitude=slice(COLOMBO_BBOX['west'], COLOMBO_BBOX['east'])\n",
    "       )\n",
    "\n",
    "    # Apply District Mask\n",
    "    try:\n",
    "        rainfall_cube = raster_analysis.mask_raster_with_vector(rainfall_cube, district_mask)\n",
    "        print(f\"Rainfall data loaded and masked: {rainfall_cube.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Masking failed ({e}), using rectangular clip\")\n",
    "    \n",
    "else:\n",
    "    print(\"CHIRPS file not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Clip OSM Buildings\n",
    "print(\"Loading OSM Buildings...\")\n",
    "osm_buildings_file = RAW_DIR / 'buildings' / 'osm_buildings.json'\n",
    "\n",
    "if osm_buildings_file.exists():\n",
    "    buildings = data_loading.load_osm_buildings(osm_buildings_file)\n",
    "    \n",
    "    # Clip to District (Masking requirement)\n",
    "    # Check if district_mask is available (defined in earlier cell)\n",
    "    if 'district_mask' in locals():\n",
    "        try:\n",
    "            print(\"Clipping buildings to district boundary...\")\n",
    "            buildings = vector_analysis.clip_vectors_to_boundary(buildings, district_mask)\n",
    "        except Exception as e:\n",
    "            print(f\"Clipping failed: {e}\")\n",
    "    \n",
    "    print(f\"Loaded {len(buildings)} buildings\")\n",
    "else:\n",
    "    print(\"OSM Buildings file not found. Please run Section 0.4.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create admin boundaries (5 sub-districts)\n",
    "print(\"Creating admin boundaries...\")\n",
    "\n",
    "ds_names = ['Colombo', 'Thimbirigasyaya', 'Dehiwala', 'Moratuwa', 'Sri Jayawardenepura Kotte']\n",
    "\n",
    "admin_boundaries = gpd.GeoDataFrame({\n",
    "    'ds_id': [f'DS{i+1:02d}' for i in range(5)],\n",
    "    'ds_name': ds_names,\n",
    "    'geometry': [\n",
    "        box(79.82, 6.90, 79.90, 7.00),\n",
    "        box(79.90, 6.90, 79.98, 7.00),\n",
    "        box(79.82, 6.82, 79.90, 6.90),\n",
    "        box(79.82, 6.75, 79.90, 6.82),\n",
    "        box(79.98, 6.90, 80.10, 7.00)\n",
    "    ]\n",
    "}, crs='EPSG:4326')\n",
    "\n",
    "print(f\"Admin boundaries: {len(admin_boundaries)} divisions\")\n",
    "print(admin_boundaries[['ds_id', 'ds_name']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the real data\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Rainfall\n",
    "rainfall_cube.mean(dim='time').plot(ax=axes[0], cmap='Blues')\n",
    "axes[0].set_title('CHIRPS 2025 Mean Monthly Rainfall (mm)')\n",
    "\n",
    "# Elevation\n",
    "elevation.plot(ax=axes[1], cmap='terrain', vmin=0, vmax=100)\n",
    "axes[1].set_title('SRTM Elevation (m)')\n",
    "\n",
    "# Admin and buildings\n",
    "admin_boundaries.plot(ax=axes[2], alpha=0.3, edgecolor='black')\n",
    "if len(buildings) < 5000:\n",
    "    buildings.plot(ax=axes[2], color='red', markersize=0.5, alpha=0.5)\n",
    "axes[2].set_title(f'Admin Boundaries & Buildings ({len(buildings)})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. NumPy Array Operations\n",
    "\n",
    "Array-based raster processing using real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rainfall as numpy array\n",
    "rainfall_np = rainfall_cube.values\n",
    "print(f\"Rainfall array shape: {rainfall_np.shape} (months, lat, lon)\")\n",
    "\n",
    "# Calculate max monthly rainfall for usage in operations\n",
    "max_monthly_rainfall = rainfall_cube.max(dim='time')\n",
    "print(f\"Max monthly rainfall range: {float(max_monthly_rainfall.min()):.1f} - {float(max_monthly_rainfall.max()):.1f} mm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operation 1: Create high rainfall mask (>300mm/month threshold)\n",
    "high_rainfall_mask = rainfall_np > 300\n",
    "print(f\"High rainfall events (>300mm/month): {high_rainfall_mask.sum()} grid-months\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_monthly_rainfall.plot(cmap='Blues', figsize=(10, 8), cbar_kwargs={'label': 'Max Monthly Rainfall (mm)'})\n",
    "plt.title('Max Monthly Rainfall (Masked)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operation 3: Calculate mean annual rainfall\n",
    "mean_annual = np.nanmean(rainfall_np, axis=0) * 12  # Convert to annual\n",
    "print(f\"Estimated annual rainfall: {np.nanmean(mean_annual):.0f} mm/year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operation 4: Normalize for vulnerability calculation\n",
    "rainfall_norm = raster_analysis.normalize_array(max_monthly_rainfall, method='minmax')\n",
    "elev_values = elevation.values\n",
    "elev_values = np.nan_to_num(elev_values, nan=np.nanmean(elev_values))\n",
    "elevation_norm = raster_analysis.normalize_array(elev_values, method='minmax')\n",
    "\n",
    "print(f\"Normalized rainfall range: {np.nanmin(rainfall_norm):.3f} - {np.nanmax(rainfall_norm):.3f}\")\n",
    "print(f\"Normalized elevation range: {np.nanmin(elevation_norm):.3f} - {np.nanmax(elevation_norm):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. PyTorch Tensor Operations\n",
    "\n",
    "GPU-aware processing with performance comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "tensor_operations.print_gpu_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to tensor\n",
    "rainfall_tensor = tensor_operations.numpy_to_tensor(max_monthly_rainfall, device='auto')\n",
    "print(f\"Tensor device: {rainfall_tensor.device}\")\n",
    "print(f\"Tensor shape: {rainfall_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Gaussian convolution for spatial smoothing\n",
    "smoothed_tensor = tensor_operations.apply_gaussian_convolution(\n",
    "    rainfall_tensor, kernel_size=5, sigma=1.5\n",
    ")\n",
    "\n",
    "# Convert back to numpy for visualization\n",
    "smoothed = tensor_operations.tensor_to_numpy(smoothed_tensor)\n",
    "\n",
    "# Compare\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axes[0].imshow(max_monthly_rainfall, cmap='Blues', origin='upper')\n",
    "axes[0].set_title('Original Max Monthly Rainfall')\n",
    "axes[1].imshow(smoothed, cmap='Blues', origin='upper')\n",
    "axes[1].set_title('Smoothed (PyTorch Gaussian Convolution)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERFORMANCE COMPARISON: NumPy vs PyTorch\n",
    "print(\"Running performance comparison...\")\n",
    "print(\"(Measuring Gaussian convolution speed)\\n\")\n",
    "\n",
    "perf_results = tensor_operations.compare_numpy_vs_torch(\n",
    "    max_monthly_rainfall, kernel_size=5, sigma=1.5, num_iterations=10\n",
    ")\n",
    "\n",
    "print(\"=\" * 55)\n",
    "print(\"        PERFORMANCE COMPARISON RESULTS\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"Array size:        {max_monthly_rainfall.shape}\")\n",
    "print(f\"Operation:         5x5 Gaussian Convolution\")\n",
    "print(f\"Iterations:        10\")\n",
    "print(\"\")\n",
    "print(f\"NumPy (scipy):     {perf_results['numpy_time']*1000:.2f} ms +/- {perf_results['numpy_std']*1000:.2f} ms\")\n",
    "print(f\"PyTorch ({perf_results['device']:6s}):  {perf_results['torch_time']*1000:.2f} ms +/- {perf_results['torch_std']*1000:.2f} ms\")\n",
    "print(\"\")\n",
    "print(f\"Speedup:           {perf_results['speedup']:.2f}x\")\n",
    "print(\"=\" * 55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Vector Processing (GeoPandas/Shapely)\n",
    "\n",
    "At least 3 vector operations as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPERATION 1: Spatial Join - assign DS to each building\n",
    "buildings_joined = vector_analysis.spatial_join_buildings_to_admin(\n",
    "    buildings, admin_boundaries, admin_id_col='ds_id'\n",
    ")\n",
    "\n",
    "print(\"OPERATION 1: Spatial Join\")\n",
    "print(f\"   Buildings with DS assignment: {len(buildings_joined)}\")\n",
    "print(buildings_joined[['building_id', 'ds_id']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPERATION 2: Building Density Calculation\n",
    "admin_with_density = vector_analysis.calculate_building_density(\n",
    "    buildings, admin_boundaries, admin_id_col='ds_id'\n",
    ")\n",
    "\n",
    "print(\"OPERATION 2: Building Density Calculation\")\n",
    "print(admin_with_density[['ds_name', 'building_count', 'building_density']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPERATION 3: Buffer analysis\n",
    "from shapely.geometry import LineString\n",
    "\n",
    "# Create sample roads\n",
    "roads = gpd.GeoDataFrame({\n",
    "    'road_id': ['R01', 'R02', 'R03'],\n",
    "    'highway': ['primary', 'secondary', 'primary'],\n",
    "    'geometry': [\n",
    "        LineString([(79.82, 6.9), (80.1, 6.9)]),\n",
    "        LineString([(79.9, 6.75), (79.9, 7.0)]),\n",
    "        LineString([(79.85, 6.85), (80.0, 6.95)])\n",
    "    ]\n",
    "}, crs='EPSG:4326')\n",
    "\n",
    "# Buffer analysis\n",
    "road_buffers = vector_analysis.create_road_buffers(\n",
    "    roads, buffer_distance=0.005, road_types=['primary', 'secondary']\n",
    ")\n",
    "\n",
    "print(\"OPERATION 3: Buffer Analysis\")\n",
    "print(f\"   Created {len(road_buffers)} road buffers\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "admin_with_density.plot(ax=ax, column='building_density', cmap='Reds', alpha=0.5, legend=True)\n",
    "road_buffers.plot(ax=ax, color='yellow', alpha=0.5)\n",
    "roads.plot(ax=ax, color='black', linewidth=2)\n",
    "if len(buildings) < 2000:\n",
    "    buildings.plot(ax=ax, color='blue', markersize=0.5, alpha=0.3)\n",
    "ax.set_title('Building Density & Road Infrastructure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Xarray Data Cubes\n",
    "\n",
    "Multi-temporal analysis with real CHIRPS 2025 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show data cube structure\n",
    "print(\"Rainfall Data Cube Structure:\")\n",
    "print(rainfall_cube)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly spatial mean\n",
    "monthly_spatial_mean = rainfall_cube.mean(dim=['latitude', 'longitude'])\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "monthly_spatial_mean.plot(marker='o')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Mean Monthly Rainfall (mm)')\n",
    "plt.title('Monthly Rainfall Pattern - Colombo District (CHIRPS 2025)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify wettest and driest months\n",
    "wettest_idx = int(monthly_spatial_mean.argmax())\n",
    "driest_idx = int(monthly_spatial_mean.argmin())\n",
    "\n",
    "print(f\"Wettest month: {rainfall_cube.time.values[wettest_idx]} ({float(monthly_spatial_mean[wettest_idx]):.1f} mm)\")\n",
    "print(f\"Driest month:  {rainfall_cube.time.values[driest_idx]} ({float(monthly_spatial_mean[driest_idx]):.1f} mm)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Raster-Vector Integration\n",
    "\n",
    "Bidirectional integration as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save rasters for integration\n",
    "import rasterio\n",
    "from rasterio.transform import from_bounds\n",
    "\n",
    "bounds = (COLOMBO_BBOX['west'], COLOMBO_BBOX['south'], \n",
    "          COLOMBO_BBOX['east'], COLOMBO_BBOX['north'])\n",
    "\n",
    "# Save rainfall raster\n",
    "h, w = max_monthly_rainfall.shape\n",
    "rainfall_path = OUTPUT_DIR / 'max_rainfall.tif'\n",
    "with rasterio.open(\n",
    "    rainfall_path, 'w', driver='GTiff',\n",
    "    height=h, width=w, count=1, dtype='float32',\n",
    "    crs='EPSG:4326', transform=from_bounds(*bounds, w, h)\n",
    ") as dst:\n",
    "    dst.write(max_monthly_rainfall.astype('float32'), 1)\n",
    "print(f\"Saved: {rainfall_path}\")\n",
    "\n",
    "# Save elevation raster  \n",
    "h2, w2 = elev_values.shape\n",
    "elevation_path = OUTPUT_DIR / 'elevation.tif'\n",
    "with rasterio.open(\n",
    "    elevation_path, 'w', driver='GTiff',\n",
    "    height=h2, width=w2, count=1, dtype='float32',\n",
    "    crs='EPSG:4326', transform=from_bounds(*bounds, w2, h2)\n",
    ") as dst:\n",
    "    dst.write(elev_values.astype('float32'), 1)\n",
    "print(f\"Saved: {elevation_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RASTER to VECTOR: Zonal Statistics\n",
    "admin_with_rainfall = integration.extract_zonal_statistics(\n",
    "    admin_boundaries, rainfall_path,\n",
    "    stats=['mean', 'max'], prefix='rainfall_'\n",
    ")\n",
    "\n",
    "admin_with_elev = integration.extract_zonal_statistics(\n",
    "    admin_with_rainfall, elevation_path,\n",
    "    stats=['mean', 'min'], prefix='elevation_'\n",
    ")\n",
    "\n",
    "print(\"RASTER to VECTOR: Zonal Statistics\")\n",
    "print(admin_with_elev[['ds_name', 'rainfall_mean', 'elevation_mean']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VECTOR to RASTER: Rasterize building density\n",
    "density_raster = integration.rasterize_vector(\n",
    "    admin_with_density,\n",
    "    value_column='building_density',\n",
    "    resolution=(-0.005, 0.005)\n",
    ")\n",
    "\n",
    "print(\"VECTOR to RASTER: Rasterized Building Density\")\n",
    "print(f\"   Shape: {density_raster.shape}\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "density_raster.plot(cmap='Oranges')\n",
    "plt.title('Rasterized Building Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Vulnerability Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all factors for vulnerability\n",
    "result = admin_with_density.merge(\n",
    "    admin_with_elev[['ds_id', 'rainfall_mean', 'rainfall_max', 'elevation_mean', 'elevation_min']],\n",
    "    on='ds_id'\n",
    ")\n",
    "\n",
    "# Normalize factors\n",
    "def normalize(series):\n",
    "    return (series - series.min()) / (series.max() - series.min() + 1e-10)\n",
    "\n",
    "rainfall_norm_v = normalize(result['rainfall_mean'])\n",
    "density_norm_v = normalize(result['building_density'])\n",
    "elev_norm_v = normalize(result['elevation_mean'])\n",
    "\n",
    "# Calculate vulnerability: V = 0.4*rainfall + 0.3*density + 0.3*(1-elevation)\n",
    "result['vulnerability_score'] = (\n",
    "    0.4 * rainfall_norm_v +\n",
    "    0.3 * density_norm_v +\n",
    "    0.3 * (1 - elev_norm_v)  # low elevation = high vulnerability\n",
    ")\n",
    "\n",
    "# Classify\n",
    "result['vulnerability_class'] = pd.cut(\n",
    "    result['vulnerability_score'],\n",
    "    bins=[0, 0.3, 0.5, 0.7, 1.0],\n",
    "    labels=['Low', 'Moderate', 'High', 'Extreme']\n",
    ")\n",
    "\n",
    "print(\"VULNERABILITY ASSESSMENT RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print(result[['ds_name', 'rainfall_mean', 'building_density', 'elevation_mean', 'vulnerability_score', 'vulnerability_class']])\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Visualization\n",
    "\n",
    "Final maps and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add required columns for visualization\n",
    "result['id'] = result['ds_id']\n",
    "\n",
    "# Create interactive vulnerability map\n",
    "vuln_map = visualization.create_vulnerability_map(\n",
    "    result,\n",
    "    value_column='vulnerability_score',\n",
    "    title='Flood Vulnerability Score'\n",
    ")\n",
    "\n",
    "# Display\n",
    "vuln_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save interactive map\n",
    "vuln_map.save(OUTPUT_DIR / 'vulnerability_map.html')\n",
    "print(f\"Saved: {OUTPUT_DIR / 'vulnerability_map.html'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ranking chart\n",
    "ranking_chart = visualization.create_vulnerability_ranking_chart(\n",
    "    result,\n",
    "    name_column='ds_name',\n",
    "    value_column='vulnerability_score',\n",
    "    top_n=10,\n",
    "    title='Vulnerability Ranking - Colombo District'\n",
    ")\n",
    "ranking_chart.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create static map for report\n",
    "fig = visualization.create_static_map(\n",
    "    result,\n",
    "    value_column='vulnerability_score',\n",
    "    title='Flood Vulnerability Assessment - Colombo District, Sri Lanka (CHIRPS 2025)',\n",
    "    cmap='YlOrRd'\n",
    ")\n",
    "\n",
    "fig.savefig(OUTPUT_DIR / 'vulnerability_map.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"Saved: {OUTPUT_DIR / 'vulnerability_map.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook demonstrated all required technical components using **real data**:\n",
    "\n",
    "| Component | Implementation | Data Source |\n",
    "|-----------|----------------|-------------|\n",
    "| **NumPy Arrays** | Masking, normalization, statistics | CHIRPS rainfall |\n",
    "| **PyTorch Tensors** | Gaussian convolution, performance comparison | Rainfall grid |\n",
    "| **Vector Processing** | Spatial join, density, buffer (3+ ops) | OSM buildings |\n",
    "| **Xarray Data Cubes** | Temporal slicing, aggregation | CHIRPS 2025 monthly |\n",
    "| **Raster-Vector Integration** | Zonal stats (R-V), rasterization (V-R) | Both |\n",
    "\n",
    "**Data Sources:**\n",
    "- SRTM DEM from AWS Open Data\n",
    "- CHIRPS 2025 monthly rainfall from UCSB\n",
    "- Building footprints from OpenStreetMap\n",
    "\n",
    "**Vulnerability Formula:**\n",
    "\n",
    "$$V = 0.4 \\times Rainfall_{norm} + 0.3 \\times BuildingDensity_{norm} + 0.3 \\times (1 - Elevation_{norm})$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}